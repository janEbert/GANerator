{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from time import time, perf_counter as pcounter, process_time as ptime\n",
    "\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch._jit_internal import weak_module, weak_script_method\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.conv import _ConvNd as ConvBase\n",
    "from torch.nn.modules.batchnorm import _BatchNorm as BatchNormBase\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as tdata\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as tfs\n",
    "import torchvision.utils as tvutils\n",
    "\n",
    "class Norm(Enum):\n",
    "    # file name parameter interpolation\n",
    "    BATCH           = 'batch'\n",
    "    VIRTUAL_BATCH   = 'virtualbatch'\n",
    "    SPECTRAL        = 'spectral'\n",
    "    INSTANCE        = 'instance'\n",
    "    AFFINE_INSTANCE = 'affineinstance'\n",
    "    NONE            = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "GANerator_parameters": true
   },
   "outputs": [],
   "source": [
    "# Paths are written in UNIX-like notation!\n",
    "# So write `C:\\Users\\user\\GANerator` as `C:/Users/user/GANerator` or `~/GANerator`.\n",
    "\n",
    "# All parameters that take classes also accept strings of the class.\n",
    "\n",
    "# Only parameters in the 'Data and Models' section will be saved and loaded!\n",
    "\n",
    "params = {\n",
    "    # Experiment specific\n",
    "    # ===================\n",
    "    'exp_name':    None,  # File names for this experiment. If `None` or `''`, `append_time` is always `True`.\n",
    "    'append_time': True,  # Append the current time to the file names (to prevent overwriting).\n",
    "    'load_dir':    '.',  # Directory to load saved files from. If `save_dir` is `None`, this also acts as `save_dir`.\n",
    "    'save_dir':    '.',  # Directory to save to. If `None`, use the value of `load_dir`.\n",
    "    \n",
    "    # Load the models and parameters from this experiment (previous `exp_name`).\n",
    "    # Also insert the optionally appended time (WIP: if this value is otherwise ambiguous).\n",
    "    # Set the parameters `models_file` or `params_file` below to use file names.\n",
    "    # If set to `True`, use `exp_name`. If `False` or `None`, do not load.\n",
    "    'load_exp':          False,\n",
    "    # Load parameters from this path. Set to `False` to not load. Priority over `load_exp`.\n",
    "    # Set to `True` to ignore this so it does not override `load_exp`.\n",
    "    'params_file':       True,\n",
    "    # Load models from this path. Set to `False` to not load. Priority over `load_exp`.\n",
    "    # Set to `True` to ignore this so it does not override `load_exp`.\n",
    "    'models_file':       True,\n",
    "    'load_weights_only': False,  # Load only the models' weights. To continue training, set this to `False`.\n",
    "    \n",
    "    'save_params':       False,  # Save the parameters in the 'Data and Models' section to a file.\n",
    "    'save_weights_only': False,  # Save only the models' weights. To continue training later, set this to `False`.\n",
    "    'checkpoint_period': 100,  # After how many steps to save a model checkpoint. Set to `0` to only save when finished.\n",
    "    \n",
    "    'num_eval_imgs': 64,  # How many images to generate for (temporal) evaluation.\n",
    "    \n",
    "    \n",
    "    # Hardware and Multiprocessing\n",
    "    # ============================\n",
    "    'num_workers':    0,  # Amount of worker threads to create on the CPU. Set to `0` to use CPU count.\n",
    "    'num_gpus':       None,  # Amount of GPUs to use. `None` to use all available ones. Set to `0` to run on CPU only.\n",
    "    'cuda_device_id': 0,  # ID of CUDA device. In most cases, this should be left at `0`.\n",
    "\n",
    "    \n",
    "    # Reproducibility\n",
    "    # ===============\n",
    "    'seed':                   0,  # Random seed if `None`. The used seed will always be saved in `saved_seed`.\n",
    "    'ensure_reproducibility': False,  # If using cuDNN: Set to `True` to ensure reproducibility in favor of performance.\n",
    "    'flush_denormals':        True,  # Whether to set denormals to zero. Some architectures do not support this.\n",
    "    \n",
    "    \n",
    "    # Data and Models\n",
    "    # ===============\n",
    "    # Only parameters in this section will be saved and updated when loading.\n",
    "    \n",
    "    # Path to the root folder of the data set. This value is only loaded if set to `None`!\n",
    "    'dataset_root':  '~/datasets/ffhq',\n",
    "    # Set this to the torchvision.datasets class (module `dsets`).\n",
    "    # This value is only loaded if set to `None`!\n",
    "    'dataset_class': dsets.ImageFolder,\n",
    "    'epochs':        5,  # Number of training epochs.\n",
    "    'batch_size':    128,  # Size of each training batch. Strongly depends on other parameters.\n",
    "    'img_channels':  3,  # Number of channels in the input images. Normally 3 for RGB and 1 for grayscale.\n",
    "    # Shape of the output images (excluding channel dimension). Can be an integer to get squares.\n",
    "    # At the moment, an image can only be square sized and a power of two.\n",
    "    'img_shape':     64,\n",
    "    'resize':        True,  # If `True`, resize images; if `False`, crop (to the center).\n",
    "    \n",
    "    'data_mean':     0.0,  # Data is normalized to this mean (per channel).\n",
    "    'data_std':      1.0,  # Data is normalized to this standard deviation (per channel).\n",
    "    'float_dtype':   torch.float32,  # Float precision as `torch.dtype`.\n",
    "    'g_input':       128,  # Size of the generator's random input vectors (`z` vector).\n",
    "    \n",
    "    # GAN hacks\n",
    "    'g_flip_labels':       False,  # Switch labels for the generator's training step.\n",
    "    'd_noisy_labels_prob': 0.0,  # Probability to switch labels when training the discriminator.\n",
    "    'smooth_labels':       False,  # Replace discrete labels with slightly different continuous ones.\n",
    "\n",
    "\n",
    "    # Values in this paragraph can be either a single value (e.g. an `int`) or a 2-`tuple` of the same type.\n",
    "    # If a single value, that value will be applied to both the discriminator and generator network.\n",
    "    # If a 2-`tuple`, the first value will be applied to the discriminator, the second to the generator.\n",
    "    'features':      64,  # Relative size of the network's internal features.\n",
    "    'optimizer':     optim.Adam,  # Optimizer class. GAN hacks recommends `(optim.SGD, optim.Adam)`.\n",
    "    # Optimizer learning rate. (Second optimizer argument, so not necessarily learning rate.)\n",
    "    'lr':            0.0002,\n",
    "    # Third optimizer argument. (For example, `betas` for `Adam` or `momentum` for `SGD`.)\n",
    "    'optim_param':   ((0.5, 0.999),),\n",
    "    # Any further optimizer keyword arguments as a dictionary.\n",
    "    'optim_kwargs':  {},\n",
    "    # Kind of normalization. Must be a `Norm` or in `('b', 'v', 's', 'i', 'a', 'n')`.\n",
    "    # Usually, spectral normalization is used in the discriminator while\n",
    "    # virtual batch normalization is used in the generator.\n",
    "    'normalization': Norm.BATCH,\n",
    "    'activation':    (nn.LeakyReLU, nn.ReLU),  # Activation between hidden layers. GAN hacks recommends `nn.LeakyReLU`.\n",
    "    # Activation keyword arguments.\n",
    "    'activation_kwargs': ({\n",
    "            'negative_slope': 0.2,\n",
    "            'inplace': True\n",
    "    }, {\n",
    "            'inplace': True\n",
    "    }),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process parameters\n",
    "\n",
    "# Model parameters as tuples\n",
    "tuple_params = (\n",
    "    'features',\n",
    "    'optimizer',\n",
    "    'lr',\n",
    "    'optim_param',\n",
    "    'optim_kwargs',\n",
    "    'normalization',\n",
    "    'activation',\n",
    "    'activation_kwargs',\n",
    ")\n",
    "\n",
    "# Parameters that we do *not* want to save (or load).\n",
    "# We list these instead of the model parameters as those should be easier to extend.\n",
    "static_params = [\n",
    "    'exp_name',\n",
    "    'append_time',\n",
    "    'load_dir',\n",
    "    'save_dir',\n",
    "\n",
    "    'load_exp',\n",
    "    'params_file',\n",
    "    'models_file',\n",
    "    'load_weights_only',\n",
    "\n",
    "    'save_params',\n",
    "    'save_weights_only',\n",
    "    'checkpoint_period',\n",
    "\n",
    "    'num_workers',\n",
    "    'num_gpus',\n",
    "    'cuda_device_id',\n",
    "\n",
    "    'seed',\n",
    "    'ensure_reproducibility',\n",
    "    'flush_denormals',\n",
    "]\n",
    "\n",
    "\n",
    "def string_to_class(string):\n",
    "    if type(string) is str:\n",
    "        string = string.split('.')\n",
    "        if len(string) == 1:\n",
    "            m = __builtins__\n",
    "        else:\n",
    "            m = globals()[string[0]]\n",
    "            for part in string[1:-1]:\n",
    "                m = getattr(m, part)\n",
    "        return getattr(m, string[-1])\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "\n",
    "# Experiment name\n",
    "\n",
    "append_time = params['append_time']\n",
    "exp_name    = params['exp_name']\n",
    "if not exp_name or append_time:\n",
    "    if exp_name is not str:\n",
    "        exp_name = ''\n",
    "    exp_name = ''.join((exp_name, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')))\n",
    "\n",
    "\n",
    "# Load parameters\n",
    "\n",
    "load_dir = params['load_dir']\n",
    "save_dir = params['save_dir']\n",
    "if save_dir is None:\n",
    "    save_dir = load_dir\n",
    "    \n",
    "load_exp = params['load_exp']\n",
    "\n",
    "params_file = params['params_file']\n",
    "load_params = params_file and (load_exp or type(params_file) is str)\n",
    "\n",
    "dataset_root  = params['dataset_root']\n",
    "dataset_class = string_to_class(params['dataset_class'])\n",
    "\n",
    "# Check whether these parameters are `None`.\n",
    "# If yes, check that parameters loading is enabled. Otherwise do not update them.\n",
    "if dataset_root is None:\n",
    "    assert load_params, '`dataset_root` cannot be `None` if not loading parameters.'\n",
    "else:\n",
    "    static_params.append('dataset_root')\n",
    "if dataset_class is None:\n",
    "    assert load_params, '`dataset_class` cannot be `None` if not loading parameters.'\n",
    "else:\n",
    "    static_params.append('dataset_class')\n",
    "\n",
    "\n",
    "if params_file and (load_exp or type(params_file) is str):\n",
    "    if type(params_file) is str:\n",
    "        params_path = Path(params_file)\n",
    "    elif type(load_exp) is bool:  # \n",
    "        params_path = Path('{}/params_{}.pt'.format(load_dir, exp_name))\n",
    "    else:\n",
    "        params_path = Path('{}/params_{}.pt'.format(load_dir, load_exp))\n",
    "\n",
    "    params_path = params_path.expanduser()\n",
    "    upd_params = torch.load(params_path)\n",
    "    params.update(upd_params)\n",
    "    del upd_params\n",
    "elif params_file == '':\n",
    "    print(\"`params_file` is an empty string (`''`). Parameters were not loaded. \"\n",
    "          'Set to `False` to suppress this warning or to `True` to let `load_exp` handle loading.')\n",
    "\n",
    "\n",
    "# Hardware and multiprocessing\n",
    "\n",
    "num_gpus       = params['num_gpus']\n",
    "cuda_device_id = params['cuda_device_id']\n",
    "if num_gpus is None:\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print('Using {} GPUs.'.format(num_gpus))\n",
    "use_gpus = num_gpus > 0\n",
    "multiple_gpus = num_gpus > 1\n",
    "if use_gpus:\n",
    "    assert torch.cuda.is_available(), 'CUDA is not available. ' \\\n",
    "            'Check what is wrong or set `num_gpus` to `0` to run on CPU.'  # Never check for this again\n",
    "    device = torch.device('cuda:' + str(cuda_device_id))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "num_workers = params['num_workers']\n",
    "if not num_workers:\n",
    "    num_workers = mp.cpu_count()\n",
    "    print('Using {} worker threads.'.format(num_workers))\n",
    "\n",
    "\n",
    "# Load model\n",
    "\n",
    "models_file = params['models_file']\n",
    "models_cp = None\n",
    "if models_file and (load_exp or type(models_file) is str):\n",
    "    if type(models_file) is str:\n",
    "        models_path = Path(models_file)\n",
    "    elif type(load_exp) is bool:\n",
    "        models_path = Path('{}/models_{}.tar'.format(load_dir, exp_name))\n",
    "    else:\n",
    "        models_path = Path('{}/models_{}.tar'.format(load_dir, load_exp))\n",
    "    models_path = models_path.expanduser()\n",
    "    models_cp = torch.load(models_path, map_location=device)\n",
    "elif models_file == '':\n",
    "    print(\"`models_file` is an empty string (`''`). Models were not loaded. \"\n",
    "          'Set to `False` to suppress this warning or to `True` to let `load_exp` handle loading.')\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "\n",
    "seed = params['seed']\n",
    "if seed is None:\n",
    "    seed = np.random.randint(10000)\n",
    "print('Seed: {}.'.format(seed))\n",
    "params['saved_seed'] = seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "ensure_reproducibility = params['ensure_reproducibility']\n",
    "torch.backends.cudnn.deterministic = ensure_reproducibility\n",
    "if ensure_reproducibility:\n",
    "    torch.backends.cudnn.benchmark = False  # This is the default but do it anyway\n",
    "    \n",
    "flush_denormals = params['flush_denormals']\n",
    "set_flush_success = torch.set_flush_denormal(flush_denormals)\n",
    "if flush_denormals and not set_flush_success:\n",
    "    print('Not able to flush denormals. `flush_denormals` set to `False`.')\n",
    "    flush_denormals = False\n",
    "\n",
    "\n",
    "# Dataset root\n",
    "\n",
    "dataset_root = Path(dataset_root).expanduser()\n",
    "\n",
    "\n",
    "# Floating point precision\n",
    "\n",
    "float_dtype = string_to_class(params['float_dtype'])\n",
    "if float_dtype is torch.float16:\n",
    "    print('PyTorch does not support half precision well yet. Be careful and assume errors.')\n",
    "torch.set_default_dtype(float_dtype)\n",
    "\n",
    "\n",
    "# Parameters we do not need to process\n",
    "\n",
    "load_weights_only = params['load_weights_only']\n",
    "save_weights_only = params['save_weights_only']\n",
    "checkpoint_period = params['checkpoint_period']\n",
    "num_eval_imgs     = params['num_eval_imgs']\n",
    "\n",
    "epochs       = params['epochs']\n",
    "batch_size   = params['batch_size']\n",
    "img_channels = params['img_channels']\n",
    "resize       = params['resize']\n",
    "\n",
    "data_mean   = params['data_mean']\n",
    "data_std    = params['data_std']\n",
    "g_input     = params['g_input']\n",
    "\n",
    "g_flip_labels       = params['g_flip_labels']\n",
    "d_noisy_labels_prob = params['d_noisy_labels_prob']\n",
    "smooth_labels       = params['smooth_labels']\n",
    "\n",
    "assert 0.0 <= d_noisy_labels_prob <= 1.0, \\\n",
    "        'Invalid probability for `d_noisy_labels`. Must be between 0 and 1 inclusively.'\n",
    "\n",
    "# Single or tuple parameters\n",
    "\n",
    "def param_as_ntuple(key, n=2):\n",
    "    val = params[key]\n",
    "    if type(val) in (tuple, list):\n",
    "        assert 0 < len(val) <= n, 'Tuples should have length {} (`{}` is `{}`).'.format(n, key, val)\n",
    "        if len(val) < n:\n",
    "            if len(val) > 1:\n",
    "                print('`{}` is `{}`. Length is less than {}; '.format(key, val, n)\n",
    "                      + 'last entry has been repeated to fit length.')\n",
    "            return tuple(val) + (val[-1],) * (n - len(val))\n",
    "        else:\n",
    "            return tuple(val)\n",
    "    return (val,) * n\n",
    "\n",
    "def ispow2(x):\n",
    "    log2 = np.log2(x)\n",
    "    return log2 == int(log2)\n",
    "\n",
    "\n",
    "img_shape = param_as_ntuple('img_shape')\n",
    "assert img_shape[0] == img_shape[1], '`img_shape` must be square (same width and height).'\n",
    "assert ispow2(img_shape[0]), '`img_shape` must be a power of two (2^n).'\n",
    "\n",
    "d_params = {}\n",
    "g_params = {}\n",
    "for key in tuple_params:\n",
    "    d_params[key], g_params[key] = param_as_ntuple(key)\n",
    "\n",
    "\n",
    "# Normalization and class parameters\n",
    "\n",
    "for p in d_params, g_params:\n",
    "    normalization = p['normalization']\n",
    "    if isinstance(normalization, str) and normalization.lower() in ('b', 'v', 's', 'i', 'a', 'n'):\n",
    "        normalization = {'b': Norm.BATCH, 'v': Norm.VIRTUAL_BATCH,\n",
    "                         's': Norm.SPECTRAL, 'i': Norm.INSTANCE,\n",
    "                         'a': Norm.AFFINE_INSTANCE, 'n': Norm.NONE}[normalization]\n",
    "    if not isinstance(normalization, Norm):\n",
    "        try:\n",
    "            normalization = Norm(normalization)\n",
    "        except ValueError:\n",
    "            normalization = string_to_class(normalization)\n",
    "        finally:\n",
    "            assert isinstance(normalization, Norm), \\\n",
    "                    \"Unknown normalization. Must be a `Norm` or in `('b', 'v', 's', 'i', 'a', 'n')`.\"\n",
    "    p['normalization'] = normalization\n",
    "\n",
    "    p['optimizer'] = string_to_class(p['optimizer'])\n",
    "    p['activation'] = string_to_class(p['activation'])\n",
    "\n",
    "\n",
    "save_models_path_str = '{}/models_{}_{{}}_steps.tar'.format(save_dir, exp_name)\n",
    "\n",
    "\n",
    "# Save parameters\n",
    "\n",
    "save_params = params['save_params']\n",
    "if save_params:\n",
    "    # We save even if we load to associate the parameters with the experiment\n",
    "    save_params_path = Path('{}/params_{}.pt'.format(save_dir, exp_name)).expanduser()\n",
    "    save_params_ = params.copy()\n",
    "    for key in static_params:\n",
    "        del save_params_[key]\n",
    "    torch.save(save_params_, save_params_path)\n",
    "    del save_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 9)  # Larger plots by default\n",
    "\n",
    "tfs_list = [\n",
    "    tfs.Resize(img_shape),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize((data_mean,) * img_channels, (data_std,) * img_channels)\n",
    "]\n",
    "if not resize:\n",
    "    tfs_list[0] = tfs.CenterCrop(img_shape)\n",
    "transform = tfs.Compose(tfs_list)\n",
    "dataset = dataset_class(dataset_root, transform=transform)\n",
    "\n",
    "dataloader = tdata.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example batch of training images\n",
    "\n",
    "show_example = True\n",
    "\n",
    "example_cols = int(np.sqrt(num_eval_imgs))\n",
    "\n",
    "try:\n",
    "    example_batch\n",
    "except NameError:\n",
    "    # Only define `example_batch` once\n",
    "    example_batch = next(iter(dataloader))\n",
    "    static_noise = torch.randn(num_eval_imgs, g_input, 1, 1, device=device).to(device, float_dtype)\n",
    "    example_noise = torch.randn(batch_size, g_input, 1, 1, device=device)\n",
    "\n",
    "if show_example:\n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.title('Example Training Batch')\n",
    "    plt.imshow(np.transpose(tvutils.make_grid(example_batch[0].to(device, float_dtype)[:num_eval_imgs],\n",
    "                                              nrow=example_cols, padding=2, normalize=True).cpu(),  # show\n",
    "                            (1, 2, 0)))  # show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model helper methods\n",
    "\n",
    "@weak_module\n",
    "class VirtualBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self.weight = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "            self.bias = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters(True)\n",
    "\n",
    "    def reset_parameters(self, all=False):\n",
    "        if self.affine:\n",
    "            nn.init.uniform_(self.weight)\n",
    "            nn.init.zeros_(self.bias)\n",
    "        if all:\n",
    "            self.in_coef = None\n",
    "            self.ref_coef = None\n",
    "\n",
    "    @weak_script_method\n",
    "    def forward(self, input, ref_batch):\n",
    "        self._check_input_dim(input)\n",
    "        if self.in_coef is None:\n",
    "            self._check_input_dim(ref_batch)\n",
    "            self.in_coef = 1 / (len(ref_batch) + 1)\n",
    "            self.ref_coef = 1 - self.in_coef\n",
    "\n",
    "        mean, std, ref_mean, ref_std = self.calculate_statistics(input, ref_batch)\n",
    "        return self.normalize(input, mean, std), self.normalize(ref_batch, ref_mean, ref_std)\n",
    "    \n",
    "    @weak_script_method\n",
    "    def calculate_statistics(self, input, ref_batch):\n",
    "        in_mean,  in_sqmean  = self.calculate_means(input)\n",
    "        ref_mean, ref_sqmean = self.calculate_means(ref_batch)\n",
    "        \n",
    "        mean   = self.in_coef * in_mean   + self.ref_coef * ref_mean\n",
    "        sqmean = self.in_coef * in_sqmean + self.ref_coef * ref_sqmean\n",
    "        \n",
    "        std     = torch.sqrt(sqmean     - mean**2     + self.eps)\n",
    "        ref_std = torch.sqrt(ref_sqmean - ref_mean**2 + self.eps)\n",
    "        return mean, std, ref_mean, ref_std\n",
    "\n",
    "    # TODO could be @staticmethod, but check @weak_script_method first\n",
    "    @weak_script_method\n",
    "    def calculate_means(self, batch):\n",
    "        mean   = torch.mean(batch,    0, keepdim=True)\n",
    "        sqmean = torch.mean(batch**2, 0, keepdim=True)\n",
    "        return mean, sqmean\n",
    "\n",
    "    @weak_script_method\n",
    "    def normalize(self, batch, mean, std):\n",
    "        return ((batch - mean) / std) * self.weight + self.bias\n",
    "    \n",
    "    @weak_script_method\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError('expected 4D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "\n",
    "\n",
    "def powers(n, b=2):\n",
    "    \"\"\"Yield `n` powers of `b` starting from `b**0`.\"\"\"\n",
    "    x = 1\n",
    "    for i in range(n):\n",
    "        x_old = x\n",
    "        x *= b\n",
    "        yield x_old, x\n",
    "\n",
    "\n",
    "def layer_with_norm(layer, norm, features):\n",
    "    if norm is Norm.BATCH:\n",
    "        return (layer, nn.BatchNorm2d(features))\n",
    "    elif norm is Norm.VIRTUAL_BATCH:\n",
    "        return (layer, VirtualBatchNorm2d(features))\n",
    "    elif norm is Norm.SPECTRAL:\n",
    "        return (nn.utils.spectral_norm(layer),)\n",
    "    elif norm is Norm.INSTANCE:\n",
    "        return (layer, nn.InstanceNorm2d(features))\n",
    "    elif norm is Norm.AFFINE_INSTANCE:\n",
    "        return (layer, nn.InstanceNorm2d(features, affine=True))\n",
    "    elif norm is Norm.NONE:\n",
    "        return (layer,)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown normalization `'{}'`\".format(norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and initialize models\n",
    "\n",
    "# Discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, normalization, activation, activation_kwargs,\n",
    "                 img_channels, img_shape, features, reference_batch=None):\n",
    "        super().__init__()\n",
    "        self.layers = self.build_layers(normalization, activation, activation_kwargs,\n",
    "                                        img_channels, img_shape, features)\n",
    "        if normalization is not Norm.VIRTUAL_BATCH:\n",
    "            self.reference_batch = None  # we can test for VBN with this invariant\n",
    "            self.layers = nn.Sequential(*self.layers)\n",
    "        elif reference_batch is None:\n",
    "            raise ValueError('Normalization is virtual batch norm, but '\n",
    "                    '`reference_batch` is `None` or missing.')\n",
    "        else:\n",
    "            self.reference_batch = reference_batch  # never `None`\n",
    "            self.layers = nn.ModuleList(self.layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_layers(norm, activation, activation_kwargs, img_channels, img_shape, features):\n",
    "        \"\"\"\n",
    "        Return a list of the layers for the discriminator network.\n",
    "\n",
    "        Example for a 64 x 64 image:\n",
    "        >>> Discriminator.build_layers(Norm.BATCH, nn.LeakyReLU, {'negative_slope': 0.2, 'inplace': True},\n",
    "                                       img_channels=3, img_shape=(64, 64), features=64)\n",
    "        [\n",
    "            # input size is 3 x 64 x 64 (given by `img_channels` and `img_shape`)\n",
    "            nn.Conv2d(img_channels, features, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size is (features) x 32 x 32\n",
    "            nn.Conv2d(features, features * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size is (features * 2) x 16 x 16\n",
    "            nn.Conv2d(features * 2, features * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size is (features * 4) x 8 x 8\n",
    "            nn.Conv2d(features * 4, features * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features * 8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size is (features * 8) x 4 x 4\n",
    "            nn.Conv2d(features * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # output size is 1 (scalar value)\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # input size is (img_channels) x (img_shape[0]) x (img_shape[1])\n",
    "        layers = [\n",
    "            nn.Conv2d(img_channels, features, 4, 2, 1, bias=False),\n",
    "            activation(**activation_kwargs)\n",
    "        ]\n",
    "        # state size is (features) x (img_shape[0] / 2) x (img_shape[1] / 2)\n",
    "        # each further layer doubles feature size and halves image size\n",
    "        for i, j in powers(int(np.log2(img_shape[0])) - 3):\n",
    "            layers.extend((\n",
    "                *layer_with_norm(nn.Conv2d(features * i, features * j, 4, 2, 1, bias=False),\n",
    "                                 norm, features * j),\n",
    "                activation(**activation_kwargs)\n",
    "            ))\n",
    "        # state size is (features * 2^n) x 4 x 4\n",
    "        layers.extend((\n",
    "            nn.Conv2d(features * j, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        ))\n",
    "        # output size is 1 (scalar value)\n",
    "        return layers\n",
    "\n",
    "    @weak_script_method\n",
    "    def forward(self, input):\n",
    "        # Separation is for performance reasons\n",
    "        if self.reference_batch is None:\n",
    "            return self.layers(input)\n",
    "        else:\n",
    "            # VBN\n",
    "            ref_batch = self.reference_batch\n",
    "            for layer in self.layers:\n",
    "                if not isinstance(layer, VirtualBatchNorm2d):\n",
    "                    input     = layer(input)\n",
    "                    ref_batch = layer(ref_batch)\n",
    "                else:\n",
    "                    input, ref_batch = layer(input, ref_batch)\n",
    "            return input\n",
    "\n",
    "\n",
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, normalization, activation, activation_kwargs,\n",
    "                 img_channels, img_shape, features, g_input, reference_batch=None):\n",
    "        super().__init__()\n",
    "        self.layers = self.build_layers(normalization, activation, activation_kwargs, img_channels, img_shape, features, g_input)\n",
    "        if normalization is not Norm.VIRTUAL_BATCH:\n",
    "            self.reference_batch = None  # we can test for VBN with this invariant\n",
    "            self.layers = nn.Sequential(*self.layers)\n",
    "        elif reference_batch is None:\n",
    "            raise ValueError('Normalization is virtual batch norm, but '\n",
    "                    '`reference_batch` is `None` or missing.')\n",
    "        else:\n",
    "            self.reference_batch = reference_batch  # never `None`\n",
    "            self.layers = nn.ModuleList(self.layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_layers(norm, activation, activation_kwargs, img_channels, img_shape, features, g_input):\n",
    "        \"\"\"\n",
    "        Return a list of the layers for the generator network.\n",
    "\n",
    "        Example for a 64 x 64 image:\n",
    "        >>> Generator.build_layers(Norm.BATCH, nn.ReLU, {'inplace': True},\n",
    "                                   img_channels=3, img_shape=(64, 64), features=64, g_input=128)\n",
    "        [\n",
    "            # input size is 128 (given by `g_input`)\n",
    "            nn.ConvTranspose2d(g_input, features * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(features * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size is (features * 8) x 4 x 4\n",
    "            nn.ConvTranspose2d(features * 8, features * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size is (features * 4) x 8 x 8\n",
    "            nn.ConvTranspose2d(features * 4, features * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size is (features * 2) x 16 x 16\n",
    "            nn.ConvTranspose2d(features * 2, features, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features),\n",
    "            nn.ReLU(True),\n",
    "            # state size is (features) x 32 x 32\n",
    "            nn.ConvTranspose2d(features, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # output size is 3 x 64 x 64 (given by `img_channels` and `img_shape`)\n",
    "        ]\n",
    "        \"\"\"\n",
    "        j = 2 ** (int(np.log2(img_shape[0])) - 3)\n",
    "        # input size is (g_input)\n",
    "        layers = [\n",
    "            *layer_with_norm(nn.ConvTranspose2d(g_input, features * j, 4, 1, 0, bias=False),\n",
    "                             norm, features * j),\n",
    "            activation(**activation_kwargs)\n",
    "        ]\n",
    "        # state size is (features * 2^n) x 4 x 4\n",
    "        # each further layer halves feature size and doubles image size\n",
    "        while j > 1:\n",
    "            i = j\n",
    "            j //= 2\n",
    "            layers.extend((\n",
    "                *layer_with_norm(nn.ConvTranspose2d(features * i, features * j, 4, 2, 1, bias=False),\n",
    "                                 norm, features * j),\n",
    "                activation(**activation_kwargs)\n",
    "            ))\n",
    "        # state size is (features) x (img_shape[0] / 2) x (img_shape[1] / 2)\n",
    "        layers.extend((\n",
    "            nn.ConvTranspose2d(features, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "        # output size is (img_channels) x (img_shape[0]) x (img_shape[1])\n",
    "        return layers\n",
    "\n",
    "    @weak_script_method\n",
    "    def forward(self, input):\n",
    "        # Separation is for performance reasons\n",
    "        if self.reference_batch is None:\n",
    "            return self.layers(input)\n",
    "        else:\n",
    "            # VBN\n",
    "            ref_batch = self.reference_batch\n",
    "            for layer in self.layers:\n",
    "                if not isinstance(layer, VirtualBatchNorm2d):\n",
    "                    input     = layer(input)\n",
    "                    ref_batch = layer(ref_batch)\n",
    "                else:\n",
    "                    input, ref_batch = layer(input, ref_batch)\n",
    "            return input\n",
    "\n",
    "\n",
    "# Initialization\n",
    "    \n",
    "def init_weights(module):\n",
    "    if isinstance(module, ConvBase):\n",
    "        nn.init.normal_(module.weight.data, 0.0, 0.02)\n",
    "    elif isinstance(module, BatchNormBase):\n",
    "        nn.init.normal_(module.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(module.bias.data, 0)\n",
    "\n",
    "\n",
    "d_net = Discriminator(d_params['normalization'], d_params['activation'], d_params['activation_kwargs'],\n",
    "                      img_channels, img_shape, d_params['features'],\n",
    "                      example_batch[0].to(device, float_dtype)).to(device, float_dtype)\n",
    "g_net = Generator(g_params['normalization'], g_params['activation'], g_params['activation_kwargs'],\n",
    "                  img_channels, img_shape, g_params['features'], g_input,\n",
    "                  example_noise.to(device, float_dtype)).to(device, float_dtype)\n",
    "\n",
    "# Load models' checkpoints\n",
    "\n",
    "if models_cp is not None:\n",
    "    d_net.load_state_dict(models_cp['d_net_state_dict'])\n",
    "    g_net.load_state_dict(models_cp['g_net_state_dict'])\n",
    "\n",
    "if multiple_gpus:\n",
    "    d_net = nn.DataParallel(d_net, list(range(num_gpus)))\n",
    "    g_net = nn.DataParallel(g_net, list(range(num_gpus)))\n",
    "\n",
    "if models_cp is None:\n",
    "    d_net.apply(init_weights)\n",
    "    g_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "d_optim_cls = d_params['optimizer']\n",
    "g_optim_cls = g_params['optimizer']\n",
    "\n",
    "d_optimizer = d_optim_cls(d_net.parameters(), d_params['lr'], d_params['optim_param'], **d_params['optim_kwargs'])\n",
    "g_optimizer = g_optim_cls(g_net.parameters(), g_params['lr'], g_params['optim_param'], **g_params['optim_kwargs'])\n",
    "\n",
    "# Load optimizers' checkpoints\n",
    "\n",
    "if models_cp is not None:\n",
    "    if not load_weights_only:\n",
    "        try:\n",
    "            d_optim_state_dict = models_cp['d_optim_state_dict']\n",
    "            g_optim_state_dict = models_cp['g_optim_state_dict']\n",
    "        except KeyError:\n",
    "            print(\"One of the optimizers' state dicts was not found; probably because \"\n",
    "                  \"only the models' weights were saved. Set `load_weights_only` to `True`.\")\n",
    "        d_optimizer.load_state_dict(d_optim_state_dict)\n",
    "        g_optimizer.load_state_dict(g_optim_state_dict)\n",
    "        d_net.train()\n",
    "        g_net.train()\n",
    "    else:\n",
    "        d_net.eval()\n",
    "        g_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for training\n",
    "\n",
    "if models_cp is not None and not load_weights_only:\n",
    "    start_epoch = models_cp['epoch']\n",
    "    steps       = models_cp['steps']\n",
    "    start_i     = models_cp['i']\n",
    "    d_losses    = models_cp['d_losses']\n",
    "    g_losses    = models_cp['g_losses']\n",
    "    eval_imgs   = models_cp['eval_imgs']\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    steps       = 0\n",
    "    start_i     = 0\n",
    "    d_losses    = []\n",
    "    g_losses    = []\n",
    "    eval_imgs   = []\n",
    "\n",
    "\n",
    "def save_checkpoint(save_models_path_str, save_weights_only, multiple_gpus,\n",
    "                    epoch, steps, i, d_losses, g_losses, eval_imgs,\n",
    "                    d_optimizer, g_optimizer, d_net, g_net):\n",
    "    if not save_weights_only:\n",
    "        save_cp = {\n",
    "            'epoch': epoch,\n",
    "            'steps': steps + 1,\n",
    "            'i': i + 1,\n",
    "            'd_losses': d_losses,\n",
    "            'g_losses': g_losses,\n",
    "            'eval_imgs': eval_imgs,\n",
    "            'd_optim_state_dict': d_optimizer.state_dict(),\n",
    "            'g_optim_state_dict': g_optimizer.state_dict(),\n",
    "        }\n",
    "    else:\n",
    "        save_cp = {}\n",
    "\n",
    "    if multiple_gpus:\n",
    "        save_cp['d_net_state_dict'] = d_net.module.state_dict()\n",
    "        save_cp['g_net_state_dict'] = g_net.module.state_dict()\n",
    "    else:\n",
    "        save_cp['d_net_state_dict'] = d_net.state_dict()\n",
    "        save_cp['g_net_state_dict'] = g_net.state_dict()\n",
    "    torch.save(save_cp, Path(save_models_path_str.format(steps + 1)).expanduser())\n",
    "\n",
    "\n",
    "def generate_labels(curr_batch_size, actual_label, switch_label,\n",
    "                    device, float_dtype, smooth_labels, noisy_labels_prob):\n",
    "    if np.random.rand() >= noisy_labels_prob:\n",
    "        label = actual_label\n",
    "    else:\n",
    "        # Flip labels\n",
    "        label = switch_label\n",
    "    labels = torch.full((curr_batch_size,), label, device=device).to(device, float_dtype)\n",
    "    if smooth_labels:\n",
    "        label_noise = torch.empty_like(labels).to(device, float_dtype).uniform_(-0.2, 0.2)\n",
    "        return labels + label_noise, label_noise\n",
    "    return labels, None\n",
    "\n",
    "\n",
    "def fill_labels(labels, label_noise, actual_label, switch_label,\n",
    "                smooth_labels, noisy_labels_prob):\n",
    "    if np.random.rand() >= noisy_labels_prob:\n",
    "        label = actual_label\n",
    "    else:\n",
    "        # Flip labels\n",
    "        label = switch_label\n",
    "    labels.fill_(label)\n",
    "    if smooth_labels:\n",
    "        label_noise.uniform_(-0.2, 0.2)\n",
    "        return labels + label_noise, label_noise\n",
    "    return labels, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "training_start         = time()\n",
    "training_start_process = ptime()\n",
    "cp_start               = pcounter()\n",
    "cp_start_process       = training_start_process\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Skip until at resume point\n",
    "        if start_i == len(dataloader):\n",
    "            start_i = 0\n",
    "            continue\n",
    "        elif i < start_i:\n",
    "            continue\n",
    "        start_i = 0\n",
    "        \n",
    "        \n",
    "        # Discriminator training step\n",
    "        # ---------------------------\n",
    "        \n",
    "        # Train with all-real batch\n",
    "        d_net.zero_grad()\n",
    "        # Format batch\n",
    "        reals = data[0].to(device, float_dtype)\n",
    "        curr_batch_size = reals.size(0)\n",
    "        labels, label_noise = generate_labels(curr_batch_size, real_label, fake_label,\n",
    "                                              device, float_dtype, smooth_labels, d_noisy_labels_prob)\n",
    "        # Classify\n",
    "        outputs = d_net(reals).view(-1)\n",
    "        # Calculate loss\n",
    "        d_loss_reals = criterion(outputs, labels)\n",
    "        # Calculate gradients\n",
    "        d_loss_reals.backward()\n",
    "        D_x = outputs.mean().item()\n",
    "        \n",
    "        # Train with all-fake batch\n",
    "        # Generate fakes\n",
    "        noise = torch.randn(curr_batch_size, g_input, 1, 1, device=device).to(device, float_dtype)\n",
    "        fakes = g_net(noise)\n",
    "        fill_labels(labels, label_noise, fake_label, real_label, smooth_labels, d_noisy_labels_prob)\n",
    "        # Classify\n",
    "        outputs = d_net(fakes.detach()).view(-1)\n",
    "        # Calculate loss\n",
    "        d_loss_fakes = criterion(outputs, labels)\n",
    "        # Calculate gradients\n",
    "        d_loss_fakes.backward()\n",
    "        D_G_z1 = outputs.mean().item()\n",
    "        \n",
    "        # Calculate total loss\n",
    "        d_loss = d_loss_reals + d_loss_fakes\n",
    "        # Update\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Generator training step\n",
    "        # -----------------------\n",
    "        \n",
    "        g_net.zero_grad()\n",
    "        # `real_label` as the actual label since the fakes are \"real\" to the generator\n",
    "        # TODO is it correct to do this here or do we do it for the discriminator output instead?\n",
    "        fill_labels(labels, label_noise, real_label, fake_label, smooth_labels, int(g_flip_labels))\n",
    "        # Use updated D for fake classification\n",
    "        outputs = d_net(fakes).view(-1)\n",
    "        # Calculate loss\n",
    "        g_loss = criterion(outputs, labels)\n",
    "        # Calculate gradients\n",
    "        g_loss.backward()\n",
    "        D_G_z2 = outputs.mean().item()\n",
    "        # Update\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Store losses\n",
    "        d_losses.append(d_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            cp_end = pcounter()\n",
    "            cp_end_process = ptime()\n",
    "            print('[{}/{}][{}/{}]\\tLoss_D: {:.4f}\\tLoss_G: {:.4f}\\t'\n",
    "                  'D(x): {:.4f}\\tD(G(z)): {:.4f} / {:.4f}\\tTime: {:.1f} s\\tPtime: {:.1f} s'.format(\n",
    "                  epoch, epochs, i, len(dataloader),\n",
    "                  d_loss.item(), g_loss.item(), D_x, D_G_z1, D_G_z2,\n",
    "                  cp_end - cp_start, cp_end_process - cp_start_process))\n",
    "            cp_start         = cp_end\n",
    "            cp_start_process = cp_end_process\n",
    "\n",
    "        # Check the generator's progress by saving its output(s) on `static_noise`\n",
    "        if steps % 500 == 0 or epoch == epochs-1 and i == len(dataloader)-1:\n",
    "            with torch.no_grad():\n",
    "                fakes = g_net(static_noise).detach().cpu()\n",
    "            eval_imgs.append(tvutils.make_grid(fakes, nrow=example_cols, padding=2, normalize=True))\n",
    "\n",
    "        # Save checkpoint\n",
    "        if checkpoint_period != 0 and steps % checkpoint_period == 0:\n",
    "            save_checkpoint(save_models_path_str, save_weights_only, multiple_gpus,\n",
    "                            epoch, steps, i, d_losses, g_losses, eval_imgs,\n",
    "                            d_optimizer, g_optimizer, d_net, g_net)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "# Save after training is finished\n",
    "save_checkpoint(save_models_path_str, save_weights_only, multiple_gpus,\n",
    "                epoch, steps, i, d_losses, g_losses, eval_imgs,\n",
    "                d_optimizer, g_optimizer, d_net, g_net)\n",
    "\n",
    "\n",
    "training_end         = time()\n",
    "training_end_process = ptime()\n",
    "\n",
    "total_time         = int(training_end - training_start)\n",
    "total_time_process = int(training_end_process - training_start_process)\n",
    "mins         = total_time // 60\n",
    "mins_process = total_time_process // 60\n",
    "\n",
    "print('{} training steps finished after {} minutes and {} seconds of real time and '\n",
    "      '{} minutes and {} seconds of process time.'.format(\n",
    "      steps - start_epoch, mins, total_time - mins * 60,\n",
    "      mins_process, total_time_process - mins_process * 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Discriminator and Generator Losses During Training')\n",
    "plt.plot(d_losses,label='D')\n",
    "plt.plot(g_losses,label='G')\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare real and fake\n",
    "\n",
    "# Use example batch from above\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis('off')\n",
    "plt.title('Real Images')\n",
    "plt.imshow(np.transpose(tvutils.make_grid(example_batch[0].to(device)[:num_eval_imgs],\n",
    "                                          nrow=example_cols, padding=2, normalize=True).cpu(),  # show\n",
    "                        (1, 2, 0)))  # show\n",
    "\n",
    "# Fake images from the last epoch\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis('off')\n",
    "plt.title('Fake Images')\n",
    "plt.imshow(np.transpose(eval_imgs[-1].cpu(), (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_animation_path = Path('{}/eval_animation_{}.gif'.format(save_dir, exp_name)).expanduser()\n",
    "\n",
    "print('{} images to be animated.'.format(len(eval_imgs)))\n",
    "\n",
    "# Animated evaluation of images on static noise\n",
    "fig = plt.figure(figsize=((13 * img_shape[0]) // 64,) * 2)  # Heuristically chosen size\n",
    "plt.axis('off')\n",
    "plt.title('Example Images During Training')\n",
    "ims = [[plt.imshow(np.transpose(img.cpu(), (1, 2, 0)), animated=True)] for img in eval_imgs]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "ani.save(save_animation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING! Can heavily slow down the browser!\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python [conda env:gans]",
   "language": "python",
   "name": "conda-env-gans-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
